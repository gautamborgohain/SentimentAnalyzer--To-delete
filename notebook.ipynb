{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.extend(['/Users/gautamborgohain/PycharmProjects/SentimentAnalyzer/src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the Twitter data. The location of the XLSX file where the tweets are going to be saved is in the config file. \n",
    "\n",
    "import get_twitter_data\n",
    "twitter = get_twitter_data.TwitterData()\n",
    "twitter.saveTweets('SMRT_Singapore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data that has been labelled\n",
    "import pandas\n",
    "data = pandas.read_excel('/Users/gautamborgohain/Desktop/data_labelled_2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed, Bad count =  1\n",
      "    User Handle                                              Tweet  \\\n",
      "1  unknownguy95  going home to exercise man! (@ tampines mrt st...   \n",
      "2    boyshawn89  it's really shagged. (@ lakeside mrt station (...   \n",
      "3  unknownguy95  that was quick (@ tampines mrt station (ew2/dt...   \n",
      "4  unknownguy95  going to tamp to catch a movie w my mum! (@ pa...   \n",
      "5      wolevilc  ............... (@ dover mrt station (ew22) - ...   \n",
      "\n",
      "   ReTweet Count In Reply To                      Created At            ID  \\\n",
      "1              0         NaN  Sat Jan 09 08:24:12 +0000 2016  6.857388e+17   \n",
      "2              0         NaN  Sat Jan 09 06:35:22 +0000 2016  6.857114e+17   \n",
      "3              0         NaN  Sat Jan 09 05:14:10 +0000 2016  6.856910e+17   \n",
      "4              0         NaN  Sat Jan 09 05:06:24 +0000 2016  6.856891e+17   \n",
      "5              0         NaN  Sat Jan 09 03:01:32 +0000 2016  6.856576e+17   \n",
      "\n",
      "                       Place Language  Sentiment_SVM  \n",
      "1     East Region, Singapore       en              0  \n",
      "2     West Region, Singapore       en              2  \n",
      "3     East Region, Singapore       en              1  \n",
      "4     East Region, Singapore       en              1  \n",
      "5  Central Region, Singapore       en              0  \n"
     ]
    }
   ],
   "source": [
    "# invoke preprocessing to get rid of the bad tweets and give a new dataframe\n",
    "import pre_processing\n",
    "pp = pre_processing.process_tweets()\n",
    "data = pp.removeBadTweets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name '__check_build'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a995d669fd08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Prepare the training data by invoking the function in the processing class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#train = p.getCountVector(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gautamborgohain/PycharmProjects/SentimentAnalyzer/src/processing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpre_processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChris_Potts_Tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0m__check_build\u001b[0m  \u001b[0;31m# avoid flakes unused variable error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '__check_build'"
     ]
    }
   ],
   "source": [
    "# Prepare the training data by invoking the function in the processing class\n",
    "\n",
    "import processing\n",
    "p = processing.processing()\n",
    "#train = p.getCountVector(data)\n",
    "# Have to wait for some time since the default invocation of this function performs SWN classification \n",
    "#and POS tagging features\n",
    "# arguments for the function : (self,frame, getSWM = True, getSubj = True, getPOSTags = True)\n",
    "#This is the already prepared one\n",
    "train = pandas.read_csv('/Users/gautamborgohain/Desktop/data_temp2.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the classifier\n",
    "\n",
    "import ML_Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "def get_hastags(tweet):\n",
    "        tweet = tweet.split('#')\n",
    "        hash_tag = []\n",
    "        for cen in tweet:\n",
    "            cen_new = cen.split(' ')\n",
    "            hash_tag.insert(0,cen_new[0])\n",
    "            for k in hash_tag:\n",
    "                if k == '':\n",
    "                    hash_tag.remove(k)\n",
    "\n",
    "        return hash_tag\n",
    "\n",
    "\n",
    "def target_features(frame):\n",
    "        tweet_targets = []\n",
    "        for tweet in frame['Tweet']:\n",
    "            tags = get_hastags(tweet)\n",
    "            keywords = ['SMRT','mrt','MRT','smrt','Singapore_MRT']\n",
    "            tokens = word_tokenize(tweet)\n",
    "            targets = []\n",
    "            for keyword in keywords:\n",
    "                if keyword in tags:  ## Thing to note here is that the words which are hash tags will be counted twice here one from tokens and one from hashtags\n",
    "                    ind = tags.index(keyword)\n",
    "                    targets.append(tags[ind])\n",
    "                elif keyword in tokens:\n",
    "                    ind = tokens.index(keyword)\n",
    "                    targets.append(tokens[ind])\n",
    "\n",
    "            tweet_targets.append(targets)\n",
    "\n",
    "        \n",
    "        return tweet_targets\n",
    "    \n",
    "tweet_target = target_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mrt', 'smrt']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweet_target)\n",
    "data['Tweet'][12]\n",
    "tweet_target[12]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
